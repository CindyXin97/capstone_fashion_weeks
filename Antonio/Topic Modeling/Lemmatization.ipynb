{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "9KujHXgwSBIi"
   },
   "source": [
    "# Topic Modeling using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DR_UMjZbSBIk",
    "outputId": "f941638e-c391-4dc0-a99d-4cefc2700fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\r\n",
      "  Downloading stanza-1.0.1-py3-none-any.whl (193 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 193 kB 2.8 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from stanza) (4.45.0)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from stanza) (3.11.4)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from stanza) (1.18.1)\r\n",
      "Requirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from stanza) (1.5.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from stanza) (2.23.0)\r\n",
      "Requirement already satisfied: six>=1.9 in /opt/conda/lib/python3.7/site-packages (from protobuf->stanza) (1.14.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf->stanza) (46.1.3.post20200325)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.3.0->stanza) (0.18.2)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->stanza) (1.24.3)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->stanza) (2.9)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->stanza) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->stanza) (2020.4.5.1)\r\n",
      "Installing collected packages: stanza\r\n",
      "Successfully installed stanza-1.0.1\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza\n",
    "\n",
    "# packages to store and manipulate data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style='white', color_codes=True)\n",
    "sns.set_context(rc={\"font.family\":\"sans\",\"font.size\":24,\"axes.titlesize\":24,\"axes.labelsize\":24})\n",
    "\n",
    "# model building package\n",
    "import sklearn\n",
    "\n",
    "# package to clean text\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from preprocessor import TwitterPreprocessor\n",
    "\n",
    "import stanza "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vapxm1-3USgO",
    "outputId": "efc413bf-37d6-4c47-b25b-86dfaed8a8e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KnnDn8elSBIx"
   },
   "outputs": [],
   "source": [
    "path = \"/kaggle/input/tweets/Hashtags_fullExport_clean.csv\"\n",
    "tweets = pd.read_csv(path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xN-4Ahmf9-Oj"
   },
   "outputs": [],
   "source": [
    "#cleaning master function\n",
    "def clean_tweet(tweet, bigrams=True):\n",
    "    tweet = re.sub(r\"RT @.+: \",'',tweet)\n",
    "    tweet = tweet.replace('#','')\n",
    "    tweet = tweet.replace('@','')\n",
    "    p = TwitterPreprocessor(tweet)\n",
    "#    # preprocess test\n",
    "    p.fully_preprocess()\n",
    "    tweet = p.text\n",
    "    return tweet\n",
    "\n",
    "tweets['clean_text'] = tweets.text.apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "6bD16lpaSBJI"
   },
   "source": [
    "We can explore the tweets a bit first."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "a6kV4Vy7SBJJ"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "zJsCckpTSBJO"
   },
   "source": [
    "We look at the text of the 10 most retweeted tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9TtEQKehSBJQ"
   },
   "outputs": [],
   "source": [
    "tweet_count = tweets[~tweets['mentions'].str.contains('OriginalFunko',na=False)].text.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "W3GBFfMbSBJV"
   },
   "source": [
    "We got rid of @OriginalFunko, as we consider it being noise, since it simply launched a giveaway of backpacks based on mentioning them. It was captured by our algorithm since #fashion was used in the tweets. We don't believe this is related anyhow to the actual fashion week phenomenon so we get rid of the tweets related to @OriginalFunko."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7vk409bFSBJW",
    "outputId": "a378e254-acb1-4b72-a214-18ed7580105d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RT @MichaelKors: Spreading the joy: @RVsmtown’s Joy stops by our Fall 2020 #MichaelKorsCollection runway show. #AllAccessKors #NYFW #_imyour_joy https://t.co/ChGzBjSjgX</th>\n",
       "      <td>6833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT @Coach: Picture perfect. Singer #PeckPalitChoke gets the artist treatment backstage at the #CoachFW20 runway show. #CoachNY #NYFW #GQxPECKxCoachFW20 #GQxPECKxCoach https://t.co/cnYot9KqmU</th>\n",
       "      <td>6192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT @BritishVogue: .@ygofficialblink's Lisa’s @Prada front-row look at #MFW was inspired by her new hairstyle: https://t.co/fBGiheLTpm https://t.co/5YAvLnAjkj</th>\n",
       "      <td>6127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT @MEENAVOGUEE: BELLA HADID FOR MARC JACOBS. #NYFW https://t.co/ylFeLZQGeB</th>\n",
       "      <td>6115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT @PopCrave: Watch @NickiMinaj’s hilarious reaction to the models at Marc Jacobs’ #NYFW show getting close to her husband. https://t.co/agjTZYTfwH</th>\n",
       "      <td>5245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT @ChinaDaily: #Blackpink member #Lisa made an appearance at the Prada Show in Milan #FashionWeek 2020. It was her first public appearance this year, her golden suit fascinated Chinese fans！#LalisaWearsPrada #LALISAxPRADAFW  @ygofficialblink https://t.co/VZvHAUXlLz</th>\n",
       "      <td>4864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT @globaltimesnews: Blackpink's #Lisa went to Japan from #MilanFashionWeek for a performance on Saturday. Her solo dance on stage during the song 'Good thing &amp; Señorita' won the hearts of many fans.  #BlackpinkinFukuoka #INYOURAREA_WORLDTOURFinale #Lalisa @ygofficialblink https://t.co/in2khel2Om</th>\n",
       "      <td>3995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT @wkorea: 오늘 밀라노로 출국한 #블랙핑크 #리사 의 공항 룩 #BLACKPINK #LISA #MilanFashionWeek #MFW  https://t.co/efONE0u6Ra</th>\n",
       "      <td>3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT @dispatchsns: 뷔(BTS), \"2019 공항패션 모음.zip\" [공항]  https://t.co/YelVNeR0Ns  #BTS #방탄소년단 #뷔 #김태형 #BTS_V #V #공항 #출국 #입국 #공항패션 #패션 #fashion #디스패치 #dispatch https://t.co/KhYJUvW5ST</th>\n",
       "      <td>2828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT @Coach: Who else is counting down the minutes to the #CoachFW20 show? #Rosé #KikoMizuhara #Jisoo #CoachNY #NYFW https://t.co/1FkgiZjZGQ</th>\n",
       "      <td>2811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "RT @MichaelKors: Spreading the joy: @RVsmtown’s...  6833\n",
       "RT @Coach: Picture perfect. Singer #PeckPalitCh...  6192\n",
       "RT @BritishVogue: .@ygofficialblink's Lisa’s @P...  6127\n",
       "RT @MEENAVOGUEE: BELLA HADID FOR MARC JACOBS. #...  6115\n",
       "RT @PopCrave: Watch @NickiMinaj’s hilarious rea...  5245\n",
       "RT @ChinaDaily: #Blackpink member #Lisa made an...  4864\n",
       "RT @globaltimesnews: Blackpink's #Lisa went to ...  3995\n",
       "RT @wkorea: 오늘 밀라노로 출국한 #블랙핑크 #리사 의 공항 룩 #BLACK...  3750\n",
       "RT @dispatchsns: 뷔(BTS), \"2019 공항패션 모음.zip\" [공항...  2828\n",
       "RT @Coach: Who else is counting down the minute...  2811"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tweet_count).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "k4jtNeovSBJe"
   },
   "source": [
    "##### Once again, the most retweeted tweets were related to K-Pop bands!\n",
    "The only two ones not being related in any way to K-Pop band members are related to supermodel Bella Hadid and singer Nicki Minaj appearances at Marc Jacob's event."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "Jrehe8mQSBJf"
   },
   "source": [
    "Let's now give a look at the <b> <font color='blue'> most common Hashtags</font></b> and <b><font color='blue'> who's being tweeted </font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Hf8uqCsISBJh",
    "outputId": "eeb9c140-3e30-4394-bf78-362aaa1c2bee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Poshmarkapp                                                                  194372\n",
       "OriginalFunko; OriginalFunko; Loungefly                                       28606\n",
       "Coach                                                                         15682\n",
       "dispatchsns                                                                   12518\n",
       "MEENAVOGUEE                                                                    8297\n",
       "globaltimesnews; ygofficialblink                                               7848\n",
       "mefeater                                                                       7352\n",
       "MichaelKors; RVsmtown                                                          6770\n",
       "PopCrave; NICKIMINAJ                                                           6184\n",
       "BritishVogue; ygofficialblink; Prada                                           6127\n",
       "MichaelKors                                                                    5869\n",
       "OriginalFunko; OriginalFunko; Loungefly; hellokitty                            5508\n",
       "ChinaDaily; ygofficialblink                                                    4864\n",
       "globaltimesnews                                                                4574\n",
       "kcrimsontide; eBay                                                             4007\n",
       "wkorea                                                                         3991\n",
       "hypebae                                                                        3957\n",
       "mirela_flory; sidharth_shukla; shivandi; ElyElena28                            3376\n",
       "SistaCafePage                                                                  3169\n",
       "hypebae; ygent_official; emrata; charlidamelio; Prada; GettyImages; Prada      2782\n",
       "Name: mentions, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['mentions'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "0e-niYpBSBJp"
   },
   "source": [
    "If we look at the tweets mentioning <i>@Poshmarkapp</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vpX5fzJESBJv",
    "outputId": "26638192-0f4d-493c-ec1e-63cdd15a1e87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"So good I had to share! Check out all the items I'm loving on @Poshmarkapp #poshmark #fashion #style #shopmycloset #luckybrand #missme #vanheusen: https://t.co/COqLOTK0nD https://t.co/f66dm4K7fQ\",\n",
       "       \"So good I had to share! Check out all the items I'm loving on @Poshmarkapp #poshmark #fashion #style #shopmycloset #michaelkors #underarmour #forever21: https://t.co/8B5szDaJQw https://t.co/lLUAGMPqMW\",\n",
       "       \"So good I had to share! Check out all the items I'm loving on @Poshmarkapp #poshmark #fashion #style #shopmycloset #madewell #freepeople #zara: https://t.co/Y4h0axAAus https://t.co/4452dSrbj9\",\n",
       "       ...,\n",
       "       \"So good I had to share! Check out all the items I'm loving on @Poshmarkapp #poshmark #fashion #style #shopmycloset #eddiebauer #turtlefur #aeropostale: https://t.co/NtsPX5rjoY https://t.co/8fWepzJaNn\",\n",
       "       \"So good I had to share! Check out all the items I'm loving on @Poshmarkapp #poshmark #fashion #style #shopmycloset #nike #loft #mia: https://t.co/qO03pS1VQl https://t.co/3smFp44RTU\",\n",
       "       \"So good I had to share! Check out all the items I'm loving on @Poshmarkapp #poshmark #fashion #style #shopmycloset #parker #express #aliceolivia: https://t.co/CHXtkVVa8O https://t.co/5GN1a4RYJ0\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[tweets['mentions']=='Poshmarkapp']['text'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "84_6YRqISBJ4"
   },
   "source": [
    "They can be considered noise, just like the @OriginalFunko ones. Let's filter both of them out and look again at the mentions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "A9RKderKSBJ9",
    "outputId": "d74adfd5-ebb4-41fc-d424-bd5e81681162"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Coach                                                                        15682\n",
       "dispatchsns                                                                  12518\n",
       "MEENAVOGUEE                                                                   8297\n",
       "globaltimesnews; ygofficialblink                                              7848\n",
       "mefeater                                                                      7352\n",
       "MichaelKors; RVsmtown                                                         6770\n",
       "PopCrave; NICKIMINAJ                                                          6184\n",
       "BritishVogue; ygofficialblink; Prada                                          6127\n",
       "MichaelKors                                                                   5869\n",
       "ChinaDaily; ygofficialblink                                                   4864\n",
       "globaltimesnews                                                               4574\n",
       "kcrimsontide; eBay                                                            4007\n",
       "wkorea                                                                        3991\n",
       "hypebae                                                                       3957\n",
       "mirela_flory; sidharth_shukla; shivandi; ElyElena28                           3376\n",
       "SistaCafePage                                                                 3169\n",
       "hypebae; ygent_official; emrata; charlidamelio; Prada; GettyImages; Prada     2782\n",
       "soompi                                                                        2534\n",
       "KOREADISPATCH2; ygofficialblink                                               2475\n",
       "eBay                                                                          2358\n",
       "Name: mentions, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[~(tweets['mentions'].str.contains('Poshmarkapp',na=False) | tweets['mentions'].str.contains('OriginalFunko', na=False))]['mentions'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "syW7vo55SBKE"
   },
   "source": [
    "## Let's move to the proper TopicModelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dLkFnKFs89jt"
   },
   "outputs": [],
   "source": [
    "tweets = tweets[~(tweets['mentions'].str.contains('Poshmarkapp',na=False) | tweets['mentions'].str.contains('OriginalFunko', na=False))]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "noPS9BVvSBKF"
   },
   "source": [
    "We already have a column containing clean text without stopwords, hashtags, links etc. from previous preprocessing we've done. \n",
    "Now we transform this into a vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yzu0A8gQSBKZ",
    "outputId": "12a23de7-c4e0-4976-8652-18127bc912bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['en', 'ja', 'ko', 'es', 'fr', 'th', 'it', 'de', 'pt', 'in', 'tr', 'pl',\n",
       "       'tl', 'ht', 'da', 'et', 'nl', 'ca', 'ar'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages_to_download = tweets[tweets['lang']!='und'].lang.value_counts()[(tweets[tweets['lang']!='und'].lang.value_counts()>1000).values].index\n",
    "languages_to_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0ncuxyxdSBKg",
    "outputId": "c7c5cef3-2b6b-47ec-a106-a0b3b1e27334"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 8.17MB/s]                    \n",
      "2020-06-04 15:06:46 INFO: Downloading default packages for language: en (English)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/en/default.zip: 100%|██████████| 402M/402M [01:01<00:00, 6.53MB/s]\n",
      "2020-06-04 15:07:55 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 7.83MB/s]                    \n",
      "2020-06-04 15:07:55 INFO: Downloading default packages for language: ja (Japanese)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/ja/default.zip: 100%|██████████| 220M/220M [00:36<00:00, 6.10MB/s]\n",
      "2020-06-04 15:08:36 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 7.85MB/s]                    \n",
      "2020-06-04 15:08:36 INFO: Downloading default packages for language: ko (Korean)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/ko/default.zip: 100%|██████████| 230M/230M [00:14<00:00, 16.4MB/s]\n",
      "2020-06-04 15:09:00 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 8.13MB/s]                    \n",
      "2020-06-04 15:09:00 INFO: Downloading default packages for language: es (Spanish)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/es/default.zip: 100%|██████████| 583M/583M [01:58<00:00, 4.93MB/s]\n",
      "2020-06-04 15:11:07 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 7.68MB/s]                    \n",
      "2020-06-04 15:11:07 INFO: Downloading default packages for language: fr (French)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/fr/default.zip: 100%|██████████| 589M/589M [01:14<00:00, 7.96MB/s]\n",
      "2020-06-04 15:12:30 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 7.71MB/s]                    \n",
      "2020-06-04 15:12:30 INFO: Downloading default packages for language: it (Italian)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/it/default.zip: 100%|██████████| 230M/230M [00:30<00:00, 7.45MB/s]\n",
      "2020-06-04 15:13:06 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 7.85MB/s]                    \n",
      "2020-06-04 15:13:06 INFO: Downloading default packages for language: de (German)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/de/default.zip: 100%|██████████| 588M/588M [01:02<00:00, 9.43MB/s]\n",
      "2020-06-04 15:14:18 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 7.98MB/s]                    \n",
      "2020-06-04 15:14:18 INFO: Downloading default packages for language: pt (Portuguese)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/pt/default.zip: 100%|██████████| 227M/227M [00:08<00:00, 25.8MB/s]\n",
      "2020-06-04 15:14:31 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 7.80MB/s]                    \n",
      "2020-06-04 15:14:31 INFO: Downloading default packages for language: tr (Turkish)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/tr/default.zip: 100%|██████████| 223M/223M [00:09<00:00, 24.5MB/s]\n",
      "2020-06-04 15:14:45 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 7.71MB/s]                    \n",
      "2020-06-04 15:14:45 INFO: Downloading default packages for language: pl (Polish)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/pl/default.zip: 100%|██████████| 228M/228M [00:24<00:00, 9.24MB/s]\n",
      "2020-06-04 15:15:14 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "languages_supported = list(languages_to_download[0:5])\n",
    "languages_supported = languages_supported+list(languages_to_download[6:9])\n",
    "languages_supported = languages_supported+list(languages_to_download[10:12])\n",
    "languages_supported\n",
    "\n",
    "for lan in languages_supported:\n",
    "    stanza.download(lan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BNvt1QAkrZZa",
    "outputId": "2eabd96c-d6b1-4f1e-d506-2756bfbeb7fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-04 15:15:16 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-06-04 15:15:17 INFO: Use device: gpu\n",
      "2020-06-04 15:15:17 INFO: Loading: tokenize\n",
      "2020-06-04 15:15:22 INFO: Loading: pos\n",
      "2020-06-04 15:15:25 INFO: Loading: lemma\n",
      "2020-06-04 15:15:26 INFO: Loading: depparse\n",
      "2020-06-04 15:15:28 INFO: Loading: ner\n",
      "2020-06-04 15:15:31 INFO: Done loading processors!\n",
      "2020-06-04 15:15:31 INFO: Loading these models for language: ja (Japanese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "2020-06-04 15:15:31 INFO: Use device: gpu\n",
      "2020-06-04 15:15:31 INFO: Loading: tokenize\n",
      "2020-06-04 15:15:31 INFO: Loading: pos\n",
      "2020-06-04 15:15:34 INFO: Loading: lemma\n",
      "2020-06-04 15:15:34 INFO: Loading: depparse\n",
      "2020-06-04 15:15:37 INFO: Done loading processors!\n",
      "2020-06-04 15:15:37 INFO: Loading these models for language: ko (Korean):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | kaist   |\n",
      "| pos       | kaist   |\n",
      "| lemma     | kaist   |\n",
      "| depparse  | kaist   |\n",
      "=======================\n",
      "\n",
      "2020-06-04 15:15:37 INFO: Use device: gpu\n",
      "2020-06-04 15:15:37 INFO: Loading: tokenize\n",
      "2020-06-04 15:15:37 INFO: Loading: pos\n",
      "2020-06-04 15:15:40 INFO: Loading: lemma\n",
      "2020-06-04 15:15:40 INFO: Loading: depparse\n",
      "2020-06-04 15:15:42 INFO: Done loading processors!\n",
      "2020-06-04 15:15:42 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2020-06-04 15:15:42 INFO: Use device: gpu\n",
      "2020-06-04 15:15:42 INFO: Loading: tokenize\n",
      "2020-06-04 15:15:42 INFO: Loading: mwt\n",
      "2020-06-04 15:15:42 INFO: Loading: pos\n",
      "2020-06-04 15:15:46 INFO: Loading: lemma\n",
      "2020-06-04 15:15:46 INFO: Loading: depparse\n",
      "2020-06-04 15:15:48 INFO: Loading: ner\n",
      "2020-06-04 15:15:54 INFO: Done loading processors!\n",
      "2020-06-04 15:15:54 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| ner       | wikiner |\n",
      "=======================\n",
      "\n",
      "2020-06-04 15:15:54 INFO: Use device: gpu\n",
      "2020-06-04 15:15:54 INFO: Loading: tokenize\n",
      "2020-06-04 15:15:54 INFO: Loading: mwt\n",
      "2020-06-04 15:15:54 INFO: Loading: pos\n",
      "2020-06-04 15:15:57 INFO: Loading: lemma\n",
      "2020-06-04 15:15:57 INFO: Loading: depparse\n",
      "2020-06-04 15:16:00 INFO: Loading: ner\n",
      "2020-06-04 15:16:06 INFO: Done loading processors!\n",
      "2020-06-04 15:16:06 INFO: Loading these models for language: it (Italian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | isdt    |\n",
      "| mwt       | isdt    |\n",
      "| pos       | isdt    |\n",
      "| lemma     | isdt    |\n",
      "| depparse  | isdt    |\n",
      "=======================\n",
      "\n",
      "2020-06-04 15:16:06 INFO: Use device: gpu\n",
      "2020-06-04 15:16:06 INFO: Loading: tokenize\n",
      "2020-06-04 15:16:06 INFO: Loading: mwt\n",
      "2020-06-04 15:16:06 INFO: Loading: pos\n",
      "2020-06-04 15:16:09 INFO: Loading: lemma\n",
      "2020-06-04 15:16:09 INFO: Loading: depparse\n",
      "2020-06-04 15:16:12 INFO: Done loading processors!\n",
      "2020-06-04 15:16:12 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| ner       | conll03 |\n",
      "=======================\n",
      "\n",
      "2020-06-04 15:16:12 INFO: Use device: gpu\n",
      "2020-06-04 15:16:12 INFO: Loading: tokenize\n",
      "2020-06-04 15:16:12 INFO: Loading: mwt\n",
      "2020-06-04 15:16:12 INFO: Loading: pos\n",
      "2020-06-04 15:16:15 INFO: Loading: lemma\n",
      "2020-06-04 15:16:15 INFO: Loading: depparse\n",
      "2020-06-04 15:16:18 INFO: Loading: ner\n",
      "2020-06-04 15:16:23 INFO: Done loading processors!\n",
      "2020-06-04 15:16:23 INFO: Loading these models for language: pt (Portuguese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | bosque  |\n",
      "| mwt       | bosque  |\n",
      "| pos       | bosque  |\n",
      "| lemma     | bosque  |\n",
      "| depparse  | bosque  |\n",
      "=======================\n",
      "\n",
      "2020-06-04 15:16:23 INFO: Use device: gpu\n",
      "2020-06-04 15:16:23 INFO: Loading: tokenize\n",
      "2020-06-04 15:16:23 INFO: Loading: mwt\n",
      "2020-06-04 15:16:23 INFO: Loading: pos\n",
      "2020-06-04 15:16:26 INFO: Loading: lemma\n",
      "2020-06-04 15:16:26 INFO: Loading: depparse\n",
      "2020-06-04 15:16:29 INFO: Done loading processors!\n",
      "2020-06-04 15:16:29 INFO: Loading these models for language: tr (Turkish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | imst    |\n",
      "| mwt       | imst    |\n",
      "| pos       | imst    |\n",
      "| lemma     | imst    |\n",
      "| depparse  | imst    |\n",
      "=======================\n",
      "\n",
      "2020-06-04 15:16:29 INFO: Use device: gpu\n",
      "2020-06-04 15:16:29 INFO: Loading: tokenize\n",
      "2020-06-04 15:16:29 INFO: Loading: mwt\n",
      "2020-06-04 15:16:29 INFO: Loading: pos\n",
      "2020-06-04 15:16:32 INFO: Loading: lemma\n",
      "2020-06-04 15:16:32 INFO: Loading: depparse\n",
      "2020-06-04 15:16:34 INFO: Done loading processors!\n",
      "2020-06-04 15:16:34 INFO: Loading these models for language: pl (Polish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | lfg     |\n",
      "| pos       | lfg     |\n",
      "| lemma     | lfg     |\n",
      "| depparse  | lfg     |\n",
      "=======================\n",
      "\n",
      "2020-06-04 15:16:34 INFO: Use device: gpu\n",
      "2020-06-04 15:16:34 INFO: Loading: tokenize\n",
      "2020-06-04 15:16:34 INFO: Loading: pos\n",
      "2020-06-04 15:16:37 INFO: Loading: lemma\n",
      "2020-06-04 15:16:37 INFO: Loading: depparse\n",
      "2020-06-04 15:16:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'en': <stanza.pipeline.core.Pipeline at 0x7f7ae5b16550>,\n",
       " 'ja': <stanza.pipeline.core.Pipeline at 0x7f7ae84a40d0>,\n",
       " 'ko': <stanza.pipeline.core.Pipeline at 0x7f7ad91a3710>,\n",
       " 'es': <stanza.pipeline.core.Pipeline at 0x7f7aed59d290>,\n",
       " 'fr': <stanza.pipeline.core.Pipeline at 0x7f7abdd06550>,\n",
       " 'it': <stanza.pipeline.core.Pipeline at 0x7f79e3c882d0>,\n",
       " 'de': <stanza.pipeline.core.Pipeline at 0x7f7b08f43450>,\n",
       " 'pt': <stanza.pipeline.core.Pipeline at 0x7f799d612510>,\n",
       " 'tr': <stanza.pipeline.core.Pipeline at 0x7f798c7589d0>,\n",
       " 'pl': <stanza.pipeline.core.Pipeline at 0x7f7942bb80d0>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelines = {}\n",
    "for l in languages_supported: \n",
    "    pipelines[l] = stanza.Pipeline(l)\n",
    "pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wGTWrqXZs6ss"
   },
   "outputs": [],
   "source": [
    "nlp = None\n",
    "\n",
    "def lemmatize(args):\n",
    "    tweet = args[0].numpy().decode('UTF-8')\n",
    "    lang = args[1].numpy().decode('UTF-8')\n",
    "\n",
    "    allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']\n",
    "    if len(tweet) <= 2: #to avoid index out of range error. Strings shorter than 2 are irrelevant in any case\n",
    "        return ''\n",
    "    if lang not in languages_supported:\n",
    "        nlp = pipelines['en']\n",
    "    else: \n",
    "        nlp = pipelines[lang]\n",
    "    doc = nlp(tweet)\n",
    "    texts_out = \" \".join([word.lemma if word.lemma not in ['-PRON-'] else '' for sent in doc.sentences for word in sent.words if word.upos in allowed_postags])\n",
    "    return texts_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uch2uoJJAu5t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22832.313759088516\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "tweets['clean_text'] = tweets['clean_text'].astype(str)\n",
    "tweets = tweets.iloc[2*int(len(tweets)/3):-1]\n",
    "args = (tf.convert_to_tensor(tweets['clean_text']), \n",
    "        tf.convert_to_tensor(tweets['lang']))\n",
    "lemmatized_tweets = tf.map_fn(lemmatize, args, dtype=tf.string)\n",
    "\n",
    "tweets['lemmatized'] = lemmatized_tweets\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MknmZx2j9TGl"
   },
   "outputs": [],
   "source": [
    "path = \"/kaggle/working/Hashtags_lemmatized_thirdThird.csv\"\n",
    "tweets.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QJ6VFH_9T_I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
